
# Improvements over plain old NN (sigmoid+quadratic cost function)

* L2-regularization (weight decay)
* Better cost function (cross-entropy) for sigmoid
* ReLU + Sigmoid
* Weight initializer to prevent saturation
* Better insight into epoch result (report precision on training data evaluation)
* Early stop
